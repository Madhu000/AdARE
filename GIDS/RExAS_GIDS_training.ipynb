{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "RExAS_GIDS_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCGoh6rcqro-"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8G8JcjOedg0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrWbJJXLsNPC"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAI1fgBDVJ3U"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "import warnings\n",
        "import pickle as pkl\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from transformers import BertConfig,BertTokenizerFast\n",
        "from transformers import TFBertModel\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "max_word_arg_head_dist=30\n",
        "ctx_len = 5\n",
        "word_embed_dim=50\n",
        "word_density=10\n",
        "max_length=60\n",
        "MAX_SEQ_LENGTH=60\n",
        "BATCH_SIZE=32\n",
        "max_word_arg_head_dist = 30\n",
        "dist_vocab_size = 2 * max_word_arg_head_dist + 1\n",
        "ignore_rel_list = ['None', 'NA', 'Other']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2tIlLAdIsDG"
      },
      "source": [
        "USE_TPU=True\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except Exception as ex:\n",
        "  print(ex)\n",
        "  USE_TPU=False\n",
        "\n",
        "print(\"        USE_TPU:\", USE_TPU)\n",
        "print(\"Eager Execution:\", tf.executing_eagerly())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uF3ZcSajIvA4"
      },
      "source": [
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkWVkPp3U2Qd"
      },
      "source": [
        "##Load Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4yTwigl7O_8"
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  mlm_model = TFBertModel.from_pretrained('bert-base-uncased',output_attentions=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJStbWX-RrX1"
      },
      "source": [
        "##Custom Layer Initialization for RExAS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GNkN9aEQUuG"
      },
      "source": [
        "class pool_sum(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, axis, **kwargs):\n",
        "        super(pool_sum, self).__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "    def build(self, input_shape):\n",
        "        super(pool_sum, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        return tf.reduce_sum(x, axis=self.axis)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:self.axis]+input_shape[self.axis+1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqVkqtBqD-Lt"
      },
      "source": [
        "class pool_mean(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, axis, **kwargs):\n",
        "        super(pool_mean, self).__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "    def build(self, input_shape):\n",
        "        super(pool_mean, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        return tf.reduce_mean(x, axis=self.axis)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:self.axis]+input_shape[self.axis+1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmdjQb5MHkgK"
      },
      "source": [
        "class stack_layer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, axis, **kwargs):\n",
        "        super(stack_layer, self).__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "    def build(self, input_shape):\n",
        "        super(stack_layer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        return tf.stack(x, axis=self.axis)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:self.axis]+input_shape[self.axis+1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxfqRBV_O48D"
      },
      "source": [
        "class Multiplication_Layer(tf.keras.layers.Layer):\n",
        "  def __init__(self,activation=None, **kwargs):\n",
        "        super(Multiplication_Layer, self).__init__(**kwargs)\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "        super(Multiplication_Layer, self).build(input_shape)\n",
        "  def call(self, x):\n",
        "        x[1]=tf.expand_dims(x[1],-1)\n",
        "        mat_mul=tf.multiply(x[0],x[1])\n",
        "        return mat_mul"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr6oISl1IaCD"
      },
      "source": [
        "class Reshape_Layer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, axis=None, **kwargs):\n",
        "        super(Reshape_Layer, self).__init__(**kwargs)\n",
        "        self.axis = axis\n",
        "    def build(self, input_shape):\n",
        "        super(Reshape_Layer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        attn_head=[]\n",
        "        for i in range(len(x)):\n",
        "          pool_attn=tf.reduce_mean(x[i],axis=1)\n",
        "          pool_attn_shape=tf.reshape(pool_attn,shape=[-1,60*60])\n",
        "          pool_attn_shape=tf.nn.softmax(pool_attn_shape)\n",
        "          attn_head.append(pool_attn_shape)\n",
        "        pool_attn_stacked=tf.stack(attn_head,axis=1)\n",
        "        reshaped_out=tf.reduce_mean(pool_attn_stacked,axis=1)\n",
        "        return reshaped_out\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[:self.axis]+input_shape[self.axis+1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGynHYcivk9u"
      },
      "source": [
        "class Attention_layer_graph(tf.keras.layers.Layer):\n",
        "  def __init__(self,head,**kawargs):\n",
        "    super(Attention_layer_graph, self).__init__()\n",
        "    self.head=head\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "    #self.w_query = self.add_weight(shape=(self.head,input_shape[0][-1],input_shape[0][-1]),initializer=\"glorot_uniform\",name=\"query_weights\",trainable=True)\n",
        "    self.bert_weight=self.add_weight(shape=(self.head,input_shape[-1],input_shape[-1]),initializer=\"glorot_uniform\",name=\"bert_query\",trainable=True)\n",
        "    #self.w_key =  self.add_weight(shape=(self.head,input_shape[-1],input_shape[-1]),initializer=\"glorot_uniform\",name=\"key_weights\",trainable=True)\n",
        "  \n",
        "  def call(self,X):\n",
        "    Y=[]\n",
        "    for i in range(self.head):\n",
        "      query=tf.matmul(X,self.bert_weight[i])\n",
        "      #key= tf.matmul(X,self.w_key[i])\n",
        "      Y.append(tf.matmul(query,X,transpose_b=True))\n",
        "    Y = tf.stack(Y,axis=1)\n",
        "    Y = Y/tf.sqrt(tf.cast(tf.shape(Y), tf.float32)[-1])\n",
        "    Y = tf.nn.softmax(Y)\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g004a18qv9MK"
      },
      "source": [
        "class Normal_adjacency(tf.keras.layers.Layer):\n",
        "    \n",
        "  def __init__(self, **kwargs):\n",
        "    super(Normal_adjacency, self).__init__()\n",
        "    \n",
        "  def call(self, A):\n",
        "    #returns vector a_prime\n",
        "    #creating degree normalized tensors from the input tensor\n",
        "    I = tf.eye(A.get_shape().as_list()[-1])\n",
        "    A=A+I\n",
        "    d1 = tf.reduce_sum(A, axis=-2)+ tf.keras.backend.epsilon()\n",
        "    #print(d1.shape)\n",
        "    d1_inv = tf.pow(d1, -0.5)\n",
        "        \n",
        "    d2 = tf.reduce_sum(A, axis=-1)+ tf.keras.backend.epsilon()\n",
        "    d2_inv = tf.pow(d2, -0.5)\n",
        "        \n",
        "    d1_inv = tf.linalg.diag(d1_inv)\n",
        "    #print(d1_inv.shape)\n",
        "    d2_inv = tf.linalg.diag(d2_inv)\n",
        "    #computing a_prime\n",
        "    a_prime = tf.matmul(d1_inv, A, transpose_a=True)\n",
        "    #print(a_prime.shape)\n",
        "    a_prime = tf.matmul(a_prime, d2_inv, transpose_a=True)\n",
        "    #a_prime = tf.eye(a_prime.get_shape().as_list()[-1]) - a_prime\n",
        "    return a_prime\n",
        "    \n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return mask\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super().get_config().copy()\n",
        "    return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM9pPXLtbSde"
      },
      "source": [
        "class Graph_Layer(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_dim,feature_regularizer=None, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        self.feature_regularizer=feature_regularizer\n",
        "        super(Graph_Layer, self).__init__(**kwargs)\n",
        "  \n",
        "  def build(self,input_shape):\n",
        "        self.fkernel = self.add_weight(name='feature_kernel',\n",
        "                                      shape=(input_shape[0][1],input_shape[1][-1], self.output_dim),\n",
        "                                      initializer='glorot_uniform',regularizer=self.feature_regularizer,trainable=True)\n",
        "        super(Graph_Layer, self).build(input_shape)\n",
        "  def call(self, x):\n",
        "        X_ = tf.keras.backend.batch_dot(x[0], x[1], axes=[-1,1])\n",
        "        mat_mult=[]\n",
        "        for j in range(x[0].shape[1]):\n",
        "          Res=tf.matmul(X_[:,j,:,:],self.fkernel[j])\n",
        "          mat_mult.append(Res)\n",
        "        Res=tf.stack(mat_mult,axis=1)\n",
        "        A1=tf.nn.relu(Res)\n",
        "        Y_=tf.reduce_sum(A1,axis=1)\n",
        "        return Y_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsAB0qjD21zQ"
      },
      "source": [
        "def get_model(max_seq_length):\n",
        "  input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int32,name=\"input_word_ids\")\n",
        "  token_type_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int32,name=\"token_type_ids\")\n",
        "  attention_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int32,name=\"attention_mask\")\n",
        "  input_entity_indicator=tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,),name=\"entity_indicator\")\n",
        "  input_entity_left_dist=tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,),name=\"entity_left_dist\")\n",
        "  input_entity_right_dist=tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,),name=\"entity_right_dist\")\n",
        "  input_entity_left_mask=tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,),name=\"entity_left_mask\")\n",
        "  input_entity_right_mask=tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,),name=\"entity_right_mask\")\n",
        "\n",
        "  entity_indicator = tf.keras.layers.Embedding(output_dim=10,input_dim=4,input_length=MAX_SEQ_LENGTH, trainable=True)(input_entity_indicator)\n",
        "  entity_indicator = tf.keras.layers.Dropout(.5)(entity_indicator)\n",
        "\n",
        "  entity_left_dist_embed=tf.keras.layers.Embedding(output_dim=5,input_dim=dist_vocab_size,input_length=MAX_SEQ_LENGTH,trainable=True)(input_entity_left_dist)\n",
        "  entity_left_dist_embed=tf.keras.layers.Dropout(.5)(entity_left_dist_embed)\n",
        "  entity_right_dist_embed=tf.keras.layers.Embedding(output_dim=5,input_dim=dist_vocab_size,input_length=MAX_SEQ_LENGTH,trainable=True)(input_entity_right_dist)\n",
        "  entity_right_dist_embed=tf.keras.layers.Dropout(.5)(entity_right_dist_embed)\n",
        "  mlm_model.layers[0].trainable=False\n",
        "  sequence_output=mlm_model([input_word_ids,token_type_ids,attention_mask])\n",
        "  embed_concat=tf.keras.layers.Concatenate()([sequence_output[0],entity_indicator,entity_left_dist_embed,entity_right_dist_embed])\n",
        "  embed_concat=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))(embed_concat)\n",
        "  adjacency_=Attention_layer_graph(4)(embed_concat)\n",
        "  normalized_adjacency_=Normal_adjacency()(adjacency_)\n",
        "  graph_out_1=Graph_Layer(256)([normalized_adjacency_,embed_concat])\n",
        "  graph_out_2=Graph_Layer(512)([normalized_adjacency_,graph_out_1])\n",
        "  entity_1=Multiplication_Layer()([embed_concat,input_entity_left_mask])\n",
        "  entity_2=Multiplication_Layer()([embed_concat,input_entity_right_mask])\n",
        "  concatenated_output=tf.keras.layers.Concatenate()([entity_1,entity_2])\n",
        "  output_1=pool_sum(axis=1)(embed_concat)\n",
        "  output_2=tf.keras.layers.Dense(512,activation=\"relu\")(output_1)\n",
        "  output_f=tf.keras.layers.Dropout(0.5)(output_2)\n",
        "  relation_out=tf.keras.layers.Dense(5,activation=\"softmax\")(output_f)\n",
        "\n",
        "  model=tf.keras.Model(inputs=[input_word_ids,token_type_ids,attention_mask,input_entity_indicator,input_entity_left_dist,input_entity_right_dist,input_entity_left_mask,input_entity_right_mask],outputs=relation_out)\n",
        "\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKfmBsHESrE1"
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  model=get_model(100)\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
        "  training_accuracy = tf.keras.metrics.CategoricalAccuracy('training_accuracy', dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSLbPmTQUwkZ"
      },
      "source": [
        "##Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXQu6TfWMgh0"
      },
      "source": [
        "def contains(sub, pri):\n",
        "    M, N = len(pri), len(sub)\n",
        "    i, LAST = 0, M-N+1\n",
        "    while True:\n",
        "        try:\n",
        "            found = pri.index(sub[0], i, LAST) # find first elem in sub\n",
        "        except ValueError:\n",
        "            return False\n",
        "        if pri[found:found+N] == sub:\n",
        "            return [found, found+N-1]\n",
        "        else:\n",
        "            i = found+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHV-Pir2U1EZ"
      },
      "source": [
        "print(\"-------------Data Preprocessing Started-------------\")\n",
        "fl_train=open(\"/content/drive/MyDrive/GIDS/gids_data/gids_train.json\",\"rb\")\n",
        "fl_valid=open(\"/content/drive/MyDrive/GIDS/gids_data/gids_dev.json\",\"rb\")\n",
        "fl_test=open(\"/content/drive/MyDrive/GIDS/gids_data/gids_test.json\",\"rb\")\n",
        "\n",
        "train_data=fl_train.readlines()\n",
        "valid_data=fl_valid.readlines()\n",
        "test_data=fl_test.readlines()\n",
        "\n",
        "\n",
        "dict_rel={}\n",
        "for i,j in enumerate(open(\"/content/drive/MyDrive/GIDS/Resource/relations_gids.txt\",\"r\")):\n",
        "  dict_rel[j.strip()]=i\n",
        "relation_out=len(dict_rel)\n",
        "\n",
        "configuration = BertConfig(num_labels=relation_out)\n",
        "\n",
        "\n",
        "def bytes_to_list(fl):\n",
        "  data=[]\n",
        "  for i,j in enumerate(fl):\n",
        "    dict_1=json.loads(j)\n",
        "    data.append(dict_1)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxAtKyzgVe6_"
      },
      "source": [
        "def get_entity_indicator(data):\n",
        "  cnt_1=0\n",
        "  cnt_2=0\n",
        "  entity_left_mask_matrix=[]\n",
        "  entity_right_mask_matrix=[]\n",
        "  entity_indicator_list=[]\n",
        "  data_text=[]\n",
        "  rel_out=[]\n",
        "  data_new=[]\n",
        "  for i,j in enumerate(tqdm(data)):\n",
        "    sent=' '.join(j['sent'])\n",
        "    sent=sent.lower()\n",
        "    words=tokenizer.tokenize(sent,add_special_tokens=True)\n",
        "    arg1_text=tokenizer.tokenize(j['sub'].lower())\n",
        "    arg2_text=tokenizer.tokenize(j['obj'].lower())\n",
        "    arg1_mask = [0]*60\n",
        "    arg2_mask = [0]*60\n",
        "    ent_ind=[3]*60\n",
        "    sub_index_1=contains(arg1_text,words)\n",
        "    sub_index_2=contains(arg2_text,words)\n",
        "    if type(sub_index_1)==bool or type(sub_index_2)==bool:\n",
        "      continue\n",
        "    data_new.append(j)\n",
        "    if len(sub_index_1)>0:\n",
        "      for k in range(sub_index_1[0],sub_index_1[1]+1):\n",
        "        if k<60:\n",
        "          arg1_mask[k]=1\n",
        "          ent_ind[k]=1\n",
        "    if len(sub_index_2)>0:\n",
        "      for k in range(sub_index_2[0],sub_index_2[1]+1):\n",
        "        if k<60:\n",
        "          arg2_mask[k]=1\n",
        "          ent_ind[k]=2\n",
        "    entity_left_mask_matrix.append(arg1_mask)\n",
        "    entity_right_mask_matrix.append(arg2_mask)\n",
        "    entity_indicator_list.append(ent_ind)\n",
        "    data_text.append(sent)\n",
        "    prop_vec=j['rel']\n",
        "    if prop_vec in dict_rel:\n",
        "      prop_index=dict_rel[prop_vec]\n",
        "      rel_out.append(prop_index)\n",
        "    elif prop_vec not in dict_rel:\n",
        "      prop_index=dict_rel['NA']\n",
        "      rel_out.append(prop_index)\n",
        "  return entity_indicator_list,entity_left_mask_matrix, entity_right_mask_matrix,data_text,rel_out,data_new                       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqwIITjUVhCh"
      },
      "source": [
        "def get_entity_dist(data):\n",
        "  entity_left_dist_matrix=[]\n",
        "  entity_right_dist_matrix=[]\n",
        "  for i,j in enumerate(tqdm(data)):\n",
        "    sent=' '.join(j['sent'])\n",
        "    words=tokenizer.tokenize(sent,add_special_tokens=True)\n",
        "    arg1_text=tokenizer.tokenize(j['sub'])\n",
        "    arg2_text=tokenizer.tokenize(j['obj'])\n",
        "    sub_index_1=contains(arg1_text,words)\n",
        "    sub_index_2=contains(arg2_text,words)\n",
        "    if type(sub_index_1)==bool or type(sub_index_2)==bool:\n",
        "      continue\n",
        "    arg1_start=sub_index_1[0]\n",
        "    arg1_end=sub_index_1[1]\n",
        "    arg2_start=sub_index_2[0]\n",
        "    arg2_end=sub_index_2[1]\n",
        "    arg1_head_dist_lst = [0]*60\n",
        "    arg2_head_dist_lst = [0]*60\n",
        "    for ind in range(0, len(words)):\n",
        "      dist = arg1_start - ind\n",
        "      if dist >= 0:\n",
        "        dist += 1\n",
        "        dist = min(dist, max_word_arg_head_dist)\n",
        "      else:\n",
        "        dist *= -1\n",
        "        dist = min(dist, max_word_arg_head_dist)\n",
        "        dist += max_word_arg_head_dist\n",
        "      if ind<60:\n",
        "        arg1_head_dist_lst[ind]=dist\n",
        "      dist = arg2_start - ind\n",
        "      if dist >= 0:\n",
        "        dist += 1\n",
        "        dist = min(dist, max_word_arg_head_dist)\n",
        "      else:\n",
        "        dist *= -1\n",
        "        dist = min(dist, max_word_arg_head_dist)\n",
        "        dist += max_word_arg_head_dist\n",
        "      if ind<60:\n",
        "        arg2_head_dist_lst[ind]=dist\n",
        "    for ind in range(arg1_start, arg1_end + 1):\n",
        "      if ind<60:\n",
        "        arg1_head_dist_lst[ind] = 1\n",
        "    for ind in range(arg2_start, arg2_end + 1):\n",
        "      if ind<60:\n",
        "        arg2_head_dist_lst[ind] = 1\n",
        "    if(len(arg1_head_dist_lst)>max_length):\n",
        "      arg1_head_dist_lst=arg1_head_dist_lst[:max_length]\n",
        "    if(len(arg2_head_dist_lst)>max_length):\n",
        "      arg2_head_dist_lst=arg2_head_dist_lst[:max_length]\n",
        "    entity_left_dist_matrix.append(arg1_head_dist_lst)\n",
        "    entity_right_dist_matrix.append(arg2_head_dist_lst)\n",
        "  return entity_left_dist_matrix, entity_right_dist_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgzV15AHEtdb"
      },
      "source": [
        "training_input=bytes_to_list(train_data)\n",
        "validation_input=bytes_to_list(valid_data)\n",
        "test_input=bytes_to_list(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWiShcVwb25S"
      },
      "source": [
        "def get_ragged_tensor_representation(encodings,entity_indicator,left_entity_dist,right_entity_dist,left_entity,right_entity):\n",
        "  encodings['input_ids']=tf.dtypes.cast(tf.constant(encodings['input_ids']),tf.int32)\n",
        "  encodings['token_type_ids']=tf.dtypes.cast(tf.constant(encodings['token_type_ids']),tf.int32)\n",
        "  encodings['attention_mask']=tf.dtypes.cast(tf.constant(encodings['attention_mask']),tf.int32)\n",
        "  entity_indicator=tf.dtypes.cast(tf.constant(entity_indicator),tf.int32)\n",
        "  left_entity_dist=tf.dtypes.cast(tf.constant(left_entity_dist),tf.int32)\n",
        "  right_entity_dist=tf.dtypes.cast(tf.constant(right_entity_dist),tf.int32)\n",
        "  left_entity=tf.dtypes.cast(tf.constant(left_entity),tf.int32)\n",
        "  right_entity=tf.dtypes.cast(tf.constant(right_entity),tf.int32)\n",
        "  return encodings,entity_indicator,left_entity_dist,right_entity_dist,left_entity,right_entity\n",
        "\n",
        "def dense_tensor_creation(data,entity_indicator,left_entity_dist,right_entity_dist,left_entity,right_entity,y):\n",
        "  input_ids=data['input_ids']\n",
        "  token_type_ids=data['token_type_ids']\n",
        "  attention_mask=data['attention_mask']\n",
        "  return (input_ids,token_type_ids,attention_mask,entity_indicator,left_entity_dist,right_entity_dist,left_entity,right_entity),y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H3Rqq6wu0sa"
      },
      "source": [
        "training_indicator,left_training_entity,right_training_entity,training_input_text,training_rel_input,training_input_=get_entity_indicator(training_input)          \n",
        "left_training_entity_dist,right_training_entity_dist=get_entity_dist(training_input)\n",
        "\n",
        "validation_indicator,left_validation_entity,right_validation_entity,validation_input_text,validation_rel_input,validation_input_=get_entity_indicator(validation_input)\n",
        "left_validation_entity_dist,right_validation_entity_dist=get_entity_dist(validation_input)\n",
        "\n",
        "test_indicator,left_test_entity,right_test_entity,test_input_text,test_rel_input,test_input_=get_entity_indicator(test_input)\n",
        "left_test_entity_dist,right_test_entity_dist=get_entity_dist(test_input)\n",
        "\n",
        "training_encodings = tokenizer(training_input_text,truncation=True,max_length=60,padding='max_length')\n",
        "training_output=tf.keras.utils.to_categorical(training_rel_input,num_classes=relation_out)\n",
        "\n",
        "validation_encodings = tokenizer(validation_input_text,truncation=True,max_length=60,padding='max_length')\n",
        "validation_output=tf.keras.utils.to_categorical(validation_rel_input,num_classes=relation_out)\n",
        "\n",
        "test_encodings = tokenizer(test_input_text,truncation=True,max_length=60,padding='max_length')\n",
        "\n",
        "\n",
        "training_input_bert,training_indicator_,left_training_entity_dist_,right_training_entity_dist_,left_training_entity_,right_training_entity_=get_ragged_tensor_representation(training_encodings,training_indicator,left_training_entity_dist,right_training_entity_dist,left_training_entity,right_training_entity)\n",
        "validation_input_bert,validation_indicator_,left_validation_entity_dist_,right_validation_entity_dist_,left_validation_entity_,right_validation_entity_=get_ragged_tensor_representation(validation_encodings,validation_indicator,left_validation_entity_dist,right_validation_entity_dist,left_validation_entity,right_validation_entity)\n",
        "test_input_bert,test_indicator_,left_test_entity_dist_,right_test_entity_dist_,left_test_entity_,right_test_entity_=get_ragged_tensor_representation(test_encodings,test_indicator,left_test_entity_dist,right_test_entity_dist,left_test_entity,right_test_entity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUt67SPlUATB"
      },
      "source": [
        "test_output=tf.keras.utils.to_categorical(test_rel_input,num_classes=relation_out)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((training_input_bert,training_indicator_,left_training_entity_dist_,right_training_entity_dist_,left_training_entity_,right_training_entity_,training_output))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((validation_input_bert,validation_indicator_,left_validation_entity_dist_,right_validation_entity_dist_,left_validation_entity_,right_validation_entity_,validation_output))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_input_bert,test_indicator_,left_test_entity_dist_,right_test_entity_dist_,left_test_entity_,right_test_entity_,test_output))\n",
        "print(\"----------Data Preprocessing Completed-------------\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyqiNVMhV07l"
      },
      "source": [
        "training_input=training_input_\n",
        "validation_input=validation_input_\n",
        "test_input=test_input_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiCoh6ZCr6Lq"
      },
      "source": [
        "def make_batches(ds,batch_size):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .map(dense_tensor_creation, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .shuffle(1000)\n",
        "      .repeat()\n",
        "      .batch(batch_size)\n",
        "      .prefetch(tf.data.AUTOTUNE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMtEu6263fxY"
      },
      "source": [
        "def make_batches_test(ds,batch_size):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .map(dense_tensor_creation, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .batch(batch_size)\n",
        "      .prefetch(tf.data.AUTOTUNE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWfZJwFC8e-o"
      },
      "source": [
        "batch_size=4\n",
        "per_replica_batch_size = batch_size\n",
        "global_batch_size=32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XzVFIxR4tTC"
      },
      "source": [
        "train_data = tpu_strategy.experimental_distribute_datasets_from_function(lambda _: make_batches(train_dataset,per_replica_batch_size))\n",
        "valid_data = tpu_strategy.experimental_distribute_datasets_from_function(lambda _: make_batches_test(valid_dataset,per_replica_batch_size))\n",
        "test_data = tpu_strategy.experimental_distribute_datasets_from_function(lambda _: make_batches_test(test_dataset,per_replica_batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m375L5eyTB_L"
      },
      "source": [
        "## GCS Bucket Path to Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnHp1s5FNEIa"
      },
      "source": [
        "checkpoint_path=\"gs://nlp_4/RExAS_GIDs/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNEV_loDTMeC"
      },
      "source": [
        "## Training ReXAS Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsB9LgpXQOxX"
      },
      "source": [
        "@tf.function\n",
        "def train_step(iterator):\n",
        "  \"\"\"The step function for one training step\"\"\"\n",
        "\n",
        "  def step_fn(inputs):\n",
        "    \"\"\"The computation to run on each TPU device.\"\"\"\n",
        "    images, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model(images, training=True)\n",
        "      loss = tf.keras.losses.categorical_crossentropy(\n",
        "          labels, logits)\n",
        "      loss = tf.nn.compute_average_loss(loss, global_batch_size=global_batch_size)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
        "    training_loss.update_state(loss * tpu_strategy.num_replicas_in_sync)\n",
        "    training_accuracy.update_state(labels, logits)\n",
        "\n",
        "  tpu_strategy.run(step_fn, args=(next(iterator),))\n",
        "\n",
        "@tf.function\n",
        "def valid_step(iterator):\n",
        "  \"\"\"The step function for one training step\"\"\"\n",
        "\n",
        "  def step_fn(inputs):\n",
        "    \"\"\"The computation to run on each TPU device.\"\"\"\n",
        "    images, labels = inputs\n",
        "    logits = model(images, training=False)\n",
        "    loss = tf.keras.losses.categorical_crossentropy(labels, logits)\n",
        "    loss = tf.nn.compute_average_loss(loss, global_batch_size=global_batch_size)\n",
        "    validation_loss.update_state(loss * tpu_strategy.num_replicas_in_sync)\n",
        "    validation_accuracy.update_state(labels, logits)\n",
        "\n",
        "  tpu_strategy.run(step_fn, args=(next(iterator),))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqJNPmSNGMo0"
      },
      "source": [
        "def get_F1(data, preds, th=0.0):\n",
        "    gt_pos = 0\n",
        "    pred_pos = 0\n",
        "    correct_pos = 0\n",
        "    for i in range(0, len(data)):\n",
        "        org_rel_name = data[i]['rel']\n",
        "        #print(\"Actual\",org_rel_name)\n",
        "        pred_val = np.argmax(preds[i])\n",
        "        pred_rel_name = list(dict_rel)[pred_val]\n",
        "        #print(preds[i].shape)\n",
        "        #print(\"Predicted\",list(dict_rel)[pred_val])\n",
        "        if org_rel_name not in ignore_rel_list:\n",
        "            gt_pos += 1\n",
        "        if pred_rel_name not in ignore_rel_list and np.max(preds[i]) > th:\n",
        "            pred_pos += 1\n",
        "        if org_rel_name == pred_rel_name and pred_rel_name not in ignore_rel_list and np.max(preds[i]) > th:\n",
        "            correct_pos += 1\n",
        "    return pred_pos, gt_pos, correct_pos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpKYxqUQCL4t"
      },
      "source": [
        "steps_per_epoch=int(len(training_input)/32)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1SqEhaw0KQM",
        "outputId": "ba960b1f-9a95-4871-c8a2-a1dc22f7a659"
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "  ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
        "  ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=2)\n",
        "  if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print('Latest checkpoint restored; Model was trained for {} steps.'.format(ckpt.optimizer.iterations.numpy()))\n",
        "  else:\n",
        "    print('Training from scratch!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training from scratch!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNZfhqlOERrJ"
      },
      "source": [
        "steps_per_eval = 10000 // batch_size\n",
        "best_dev_acc=-1.0\n",
        "count=0\n",
        "test_f1_score=[]\n",
        "train_iterator = iter(train_data)\n",
        "valid_iterator = iter(valid_data)\n",
        "test_iterator = iter(test_data)\n",
        "for epoch in range(80):\n",
        "  print('Epoch: {}/80'.format(epoch+1))\n",
        "  start_time = datetime.datetime.now()\n",
        "  for step in range(steps_per_epoch):\n",
        "    train_step(train_iterator)\n",
        "  print('Current step: {}, training loss: {}, accuracy: {}%'.format(\n",
        "      optimizer.iterations.numpy(),\n",
        "      round(float(training_loss.result()), 4),\n",
        "      round(float(training_accuracy.result()) * 100, 2)))\n",
        "  training_loss.reset_states()\n",
        "  training_accuracy.reset_states()\n",
        "  end_time=datetime.datetime.now()\n",
        "  print(\"Time taken for training is {}s\\n\".format((end_time-start_time).total_seconds()))\n",
        "  print(\"------Validation DataSet Performance------\")\n",
        "  prediction_valid=model.predict(valid_data,steps=int(len(validation_input)/32)+1,verbose=1)\n",
        "  pred_pos, gt_pos, correct_pos = get_F1(validation_input, prediction_valid)\n",
        "  p = float(correct_pos) / (pred_pos + 1e-8)\n",
        "  r = float(correct_pos) / (gt_pos +  1e-8)\n",
        "  dev_acc = (2 * p * r) / (p + r +  1e-8)\n",
        "  print(\"Now Validation Precision is {}, Recall is {},  F1-Score is  {}\".format(round(p,4),round(r,4),round(dev_acc,4)))\n",
        "  print(\"------Test DataSet Performance------\")\n",
        "  prediction=model.predict(test_data,steps=int(len(test_input)/32)+1,verbose=1)\n",
        "  pred_pos, gt_pos, correct_pos = get_F1(test_input, prediction)\n",
        "  #print(pred_pos, '\\t', gt_pos, '\\t', correct_pos)\n",
        "  p = float(correct_pos) / (pred_pos+ 1e-8)\n",
        "  r = float(correct_pos) / (gt_pos+ 1e-8)\n",
        "  test_acc = (2 * p * r) / (p + r+ 1e-8)\n",
        "  print(\"Now test Precision is {}, Recall is {},  F1-Score is  {}\".format(round(p,4),round(r,4),round(test_acc,4)))\n",
        "  if test_acc>best_dev_acc:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saved checkpoint for step {}: {}\".format(int(ckpt.optimizer.iterations.numpy()), ckpt_save_path))\n",
        "    best_dev_acc=test_acc\n",
        "    count=0\n",
        "  elif test_acc<=best_dev_acc:\n",
        "    count=count+1\n",
        "  test_f1_score.append(test_acc)\n",
        "  if count==10:\n",
        "    print(\"Last 10 epochs F1-score didn't improve for validation dataset Training completed\")\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}